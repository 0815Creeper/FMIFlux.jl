{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NeuralODEs in real life applications\n",
    "-----\n",
    "Tutorial by Tobias Thummerer | Last edit: 07-21-2023\n",
    "\n",
    "This workshop was held at the JuliaCon 2023 | 07-25-2023 | MIT (Boston, USA)\n",
    "\n",
    "Keywords: *#NeuralODE, #NeuralFMU, #PeNODE, #HybridModeling*\n",
    "\n",
    "## Introduction\n",
    "NeuralODEs lead to amazing results in academic examples. But the expectations are often being disappointed as soon as one tries to adapt this concept for real life use cases. Bad convergence behavior, handling of discontinuities and/or instabilities are just some of the stumbling blocks that might pop up during the first steps. During the workshop, we want to show how to integrate real life industrial models in NeuralODEs using FMI and present sophisticated training strategies.\n",
    "\n",
    "This tutorial can be used in two ways:\n",
    "1. As a single script, showing how a NeuralFMU can be setup and trained. Results can be loaded from a precomputed hyperparameter optimization.\n",
    "2. As a module (see sections *Optional: Organize as module*) together with the file `juliacon_2023_distributedhyperopt.jl` to perform your own distributed hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Tobias Thummerer, Lars Mikelsons\n",
    "# Licensed under the MIT license. \n",
    "# See LICENSE (https://github.com/thummeto/FMIFlux.jl/blob/main/LICENSE) file in the project root for details.\n",
    "\n",
    "# This workshop was held at the JuliaCon2023 @ MIT (Boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Organize as module\n",
    "If you want, you can place all code inside of a module named `NODE_Training`, this simplifies hyper parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hyper parameter optimization, place the code in a `module`\n",
    "# just uncomment the following lines (and the one at the very end, too!) \n",
    "#module NODE_Training \n",
    "#using DistributedHyperOpt\n",
    "#using DistributedHyperOpt.Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Packages\n",
    "Before we start modeling our NeuralODE, we load all required packages. If some packages are still missing, install them by typing `import Pkg; Pkg.add(\"[PKG-NAME]\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the required libraries\n",
    "using FMIFlux       # for NeuralFMUs\n",
    "using FMI           # import FMUs into Julia \n",
    "using FMIZoo        # a collection of demo models, including the VLDM\n",
    "using FMIFlux.Flux  # Machine Learning in Julia\n",
    "\n",
    "import JLD2         # data format for saving/loading parameters\n",
    "\n",
    "import Random       # for fixing the random seed\n",
    "using Plots         # plotting results\n",
    "\n",
    "# A variable indicating the hyperparameter run\n",
    "HPRUN = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside the packages, we use another little script that includes some nice plotting functions specialy for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper file with some predefined functions to make \"things look nicer\", but are not really relevant to the topic\n",
    "include(joinpath(@__DIR__, \"juliacon_2023_helpers.jl\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because notebooks can't handle progress bars, we disable *progress bar printing* - but feel free to enable it if you are using the code outside of a jupyter notebook. The progress bar gives further helpful information, like the estimated remaining computation time for simulation and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars in jupyter notebook\n",
    "showProgress=false;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading FMU & Data\n",
    "Before starting with hybrid modeling, we load in the used training data and our FMU of the VLDM. We simulate the FMU, plot the results and compare them to data. \n",
    "\n",
    "We start by loading in the data (training and validation) used in this tutorial from *FMIZoo.jl* - a container library for different system model FMUs and corresponding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample length for data is 0.1s \n",
    "dt = 0.1 \n",
    "\n",
    "# load data (training and validation) from FMIZoo.jl, gather simulation parameters for FMU\n",
    "data = VLDM(:train, dt=dt) \n",
    "data_validation = VLDM(:validate, dt=dt)\n",
    "\n",
    "# start (`tStart`) and stop time (`tStop`) for simulation, saving time points for ODE solver (`tSave`)\n",
    "tStart = data.consumption_t[1]\n",
    "tStop = data.consumption_t[end]\n",
    "tSave = data.consumption_t\n",
    "\n",
    "# have a look on the FMU parameters (these are the file paths to the characteristic maps, remaining parameters are set to default by the FMU)\n",
    "display(data.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we load the FMU and have a look on its model meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our FMU of the VLDM (we take it from the FMIZoo.jl, exported with Dymola 2020x)\n",
    "fmu = fmiLoad(\"VLDM\", \"Dymola\", \"2020x\"; type=:ME, logLevel=:info)  # \"Log everything that might be interesting!\", default is `:warn`\n",
    "\n",
    "# let's have a look on the model meta data\n",
    "fmiInfo(fmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating is as easy as calling `fmiSimulate`. Note, that we are putting in the paramter dictionary `data.params` from above. This FMU has many events, these are detected and handled automatically by *FMI.jl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run a simulation from `tStart` to `tStop`, use the parameters we just viewed for the simulation run\n",
    "resultFMU = fmiSimulate(fmu, (tStart, tStop); parameters=data.params, showProgress=showProgress)\n",
    "display(resultFMU)\n",
    "\n",
    "# Plot the simulation results\n",
    "fig = plot(resultFMU)                                                                        # Plot it, but this is a bit too much, so ...\n",
    "fig = plot(resultFMU; stateIndices=6:6)                                                      # ... only plot the state #6 (the cumulative consumption) and ...\n",
    "fig = plot(resultFMU; stateIndices=6:6, ylabel=\"Cumulative consumption [Ws]\", label=\"FMU\")   # ... add some helpful labels!\n",
    "\n",
    "# further plot the (measurement) data values `consumption_val` and deviation between measurements `consumption_dev`\n",
    "plot!(fig, data.cumconsumption_t, data.cumconsumption_val; label=\"Data\", ribbon=data.cumconsumption_dev, fillalpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do another simulation run and record the derivative values, to have a good starting point for scaling the pre- and post-processing layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable we want to manipulate - why we are picking exactly these three is shown a few lines later ;-)\n",
    "manipulatedDerVars = [\"der(dynamics.accelerationCalculation.integrator.y)\",\n",
    "                      \"der(dynamics.accelerationCalculation.limIntegrator.y)\",\n",
    "                      \"der(result.integrator.y)\"]\n",
    "# alternative: manipulatedDerVars = fmu.modelDescription.derivativeValueReferences[4:6]\n",
    "\n",
    "# reference simulation to record the derivatives \n",
    "resultFMU = fmiSimulate(fmu, (tStart, tStop), parameters=data.params, recordValues=:derivatives, saveat=tSave, showProgress=showProgress) \n",
    "manipulatedDerVals = fmiGetSolutionValue(resultFMU, manipulatedDerVars)\n",
    "\n",
    "# what happens without propper transformation between FMU- and ANN-domain?\n",
    "plot(resultFMU.values.t, manipulatedDerVals[1,:][1]; label=\"vehicle velocity [m/s]\");\n",
    "plot!(resultFMU.values.t, tanh.(manipulatedDerVals[1,:][1]); label=\"tanh(velocity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's why we need shift- and scale layers!\n",
    "\n",
    "Finally, the FMU is unloaded again to release memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload FMU\n",
    "fmiUnload(fmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NeuralFMU setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NeuralFMU](https://github.com/thummeto/FMIFlux.jl/blob/main/docs/src/examples/img/juliacon_2023/neuralfmu_topology.png?raw=true)\n",
    "\n",
    "Equipped with data and a simulation model, we can setup the NeuralFMU as introduced in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that builds the considered NeuralFMU on basis of a given FMU (FMI-Version 2.0) `f`\n",
    "function build_NFMU(f::FMU2)\n",
    "    \n",
    "    # pre- and post-processing\n",
    "    preProcess = ShiftScale(manipulatedDerVals)\n",
    "    preProcess.scale[:] *= 0.25 # add some additional \"buffer\"\n",
    "    postProcess = ScaleShift(preProcess; indices=2:3) \n",
    "\n",
    "    # cache\n",
    "    cache = CacheLayer()\n",
    "    cacheRetrieve = CacheRetrieveLayer(cache)\n",
    "\n",
    "    # the gates layer, signals are:\n",
    "    # (1) acceleration from FMU (gate=1.0 | open)\n",
    "    # (2) consumption  from FMU (gate=1.0 | open)\n",
    "    # (3) acceleration from ANN (gate=0.0 | closed)\n",
    "    # (4) consumption  from ANN (gate=0.0 | closed)\n",
    "    # the acelerations [1,3] and consumptions [2,4] are paired\n",
    "    gates = ScaleSum([1.0, 1.0, 0.0, 0.0], [[1,3], [2,4]]) \n",
    "\n",
    "    # setup the NeuralFMU topology\n",
    "    net = Chain(x -> f(; x=x),                  # take `x`, put it into the FMU, retrieve `dx`\n",
    "            dx -> cache(dx),                    # cache `dx`\n",
    "            dx -> dx[4:6],                      # forward only dx[4, 5, 6]\n",
    "            preProcess,                         # pre-process `dx`\n",
    "            Dense(3, 32, tanh),                 # Dense Layer 3 -> 32 with `tanh` activation\n",
    "            Dense(32, 2, tanh),                 # Dense Layer 32 -> 2 with `tanh` activation \n",
    "            postProcess,                        # post process `dx`\n",
    "            dx -> cacheRetrieve(5:6, dx),       # dynamics FMU | dynamics ANN\n",
    "            gates,                              # compute resulting dx from ANN + FMU\n",
    "            dx -> cacheRetrieve(1:4, dx))       # stack together: dx[1,2,3,4] from cache + dx[5,6] from gates\n",
    "\n",
    "    # new NeuralFMU \n",
    "    neuralFMU = ME_NeuralFMU(f, net, (tStart, tStop); saveat=tSave)\n",
    "    neuralFMU.modifiedState = false # speed optimization (NeuralFMU state equals FMU state)\n",
    "    \n",
    "    return neuralFMU \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the NeuralFMU: First, load the FMU und built a NeuralFMU from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our FMU (we take one from the FMIZoo.jl, exported with Dymola 2022x)\n",
    "fmu = fmiLoad(\"VLDM\", \"Dymola\", \"2020x\"; type=:ME)\n",
    "\n",
    "# build NeuralFMU\n",
    "neuralFMU = build_NFMU(fmu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, do a simulation for a given start state `x0` from *FMIZoo.jl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start state vector from data (FMIZoo)\n",
    "x0 = FMIZoo.getStateVector(data, tStart)\n",
    "\n",
    "# simulate and plot the (uninitialized) NeuralFMU\n",
    "resultNFMU = neuralFMU(x0,                          # the start state to solve the ODE\n",
    "                       (tStart, tStop);             # the simulation range\n",
    "                       parameters=data.params,      # the parameters for the VLDM\n",
    "                       showProgress=showProgress)   # show progress (or not)\n",
    "\n",
    "display(resultNFMU)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look on the cumulative consumption plot ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the NeuralFMU, original FMU and data (cumulative consumption)\n",
    "fig = plot(resultNFMU; stateIndices=6:6, stateEvents=false, timeEvents=false, label=\"NeuralFMU (untrained)\", ylabel=\"cumulative consumption [Ws]\")\n",
    "plot!(fig, resultFMU; stateIndices=6:6, values=false, stateEvents=false, timeEvents=false, label=\"FMU\")\n",
    "plot!(fig, data.cumconsumption_t, data.cumconsumption_val, label=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, unload the FMU and invalidate the NeuralFMU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload FMU / invalidate NeuralFMU\n",
    "fmiUnload(fmu)\n",
    "neuralFMU = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the NeuralFMU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An untrained NeuralFMU is not that impressive - so let's train it a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data \n",
    "train_t = data.consumption_t \n",
    "\n",
    "# data is as \"array of arrays\" required (multidimensional data)\n",
    "train_data = collect([d] for d in data.cumconsumption_val);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the loss function is defined. The loss is computed on basis of a given `solution` and `data`. Dependent on the hyperparameter `LOSS`, either `:MAE` or `:MSE` is used to compute the loss. The hyperparameter `LASTWEIGHT` determines how much the last solution point is weight against the remaining solution points. For example a value of $0.3$ determines that the last point of the solution contributes $30\\%$ to the loss, whereas all remaining solution points contribute $70\\%$ in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function _lossFct(solution::FMU2Solution, data::VLDM_Data, LOSS::Symbol, LASTWEIGHT::Real)\n",
    "\n",
    "    if !solution.success\n",
    "        @warn \"The solution process was not successfull, try to compute loss anyway.\"\n",
    "    end\n",
    "\n",
    "    # determine the indices in the data array (sampled with 10Hz)\n",
    "    ts = 1+round(Int, solution.states.t[1]/dt)\n",
    "    te = 1+round(Int, solution.states.t[end]/dt)\n",
    "    \n",
    "    nfmu_cumconsumption = fmiGetSolutionState(solution, 6; isIndex=true)\n",
    "    cumconsumption = data.cumconsumption_val[ts:te]\n",
    "    cumconsumption_dev = data.cumconsumption_dev[ts:te]\n",
    "\n",
    "    Δcumconsumption = 0.0\n",
    "    if LOSS == :MAE\n",
    "        Δcumconsumption = FMIFlux.Losses.mae_last_element_rel_dev(nfmu_cumconsumption, cumconsumption, cumconsumption_dev, LASTWEIGHT)\n",
    "    elseif LOSS == :MSE\n",
    "        Δcumconsumption = FMIFlux.Losses.mse_last_element_rel_dev(nfmu_cumconsumption, cumconsumption, cumconsumption_dev, LASTWEIGHT)\n",
    "    else\n",
    "        @assert false, \"Unknown LOSS: `$(LOSS)`\"\n",
    "    end\n",
    "    \n",
    "    return Δcumconsumption \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the function `train!` is defined, that triggers a new training run for a given set of hyperparameters `hyper_params`, a training ressource `ressource` and the current training index `ind`. The following hyperparameters are used:\n",
    "- `TRAINDUR` equals the given `ressource` and specifies the training duration (meassured on data) in seconds. Hyperparameter optimizers like *Hyperband* use different ressources during searching for a minimum.\n",
    "- `ETA` the update rate $\\eta$ of the *Adam* optimizer.\n",
    "- `BETA1` the first momentum coefficient $\\beta_1$ of the *Adam* optimizer.\n",
    "- `BETA2` the second momentum coefficient $\\beta_2$ of the *Adam* optimizer. \n",
    "- `BATCHDUR` the duration of a single batch element (length) in seconds.\n",
    "- `LASTWEIGHT` a weighting factor between the last solution point and all remaining solution points. \n",
    "- `SCHEDULER` an identifier for the batch scheduler, can be `:Sequential`, `:Random` or `:LossAccumulation`.\n",
    "- `LOSS` an identifier for the loss function to use, `:MAE` or `:MSE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ressource = training time horizon (duration of data seen)\n",
    "function train!(hyper_params, ressource, ind)\n",
    "\n",
    "    # make the runs determinisitic by fixing the random seed\n",
    "    Random.seed!(1234)\n",
    "\n",
    "    # training duration (in seconds) equals the given ressource\n",
    "    TRAINDUR = ressource\n",
    "\n",
    "    # unpack the hyperparemters\n",
    "    ETA, BETA1, BETA2, BATCHDUR, LASTWEIGHT, SCHEDULER, LOSS = hyper_params\n",
    "\n",
    "    # compute the number of training steps TRAINDUR / BATCHDUR, but do at least one step\n",
    "    steps = max(round(Int, TRAINDUR/BATCHDUR), 1) \n",
    "\n",
    "    # print a bit of info\n",
    "    @info \"--------------\\nStarting run $(ind) with parameters: $(hyper_params) and ressource $(ressource) doing $(steps) step(s).\\n--------------------\"\n",
    "\n",
    "    # load our FMU (we take one from the FMIZoo.jl, exported with Dymola 2020x)\n",
    "    fmu = fmiLoad(\"VLDM\", \"Dymola\", \"2020x\"; type=:ME) \n",
    "\n",
    "    # built the NeuralFMU on basis of the loaded FMU `fmu`\n",
    "    neuralFMU = build_NFMU(fmu)\n",
    "\n",
    "    # switch to a more efficient execution configuration, allocate only a single FMU instance, see:\n",
    "    # https://thummeto.github.io/FMI.jl/dev/features/#Execution-Configuration\n",
    "    fmu.executionConfig = FMI.FMIImport.FMU2_EXECUTION_CONFIGURATION_NOTHING\n",
    "    c, _ = FMIFlux.prepareSolveFMU(neuralFMU.fmu, nothing, neuralFMU.fmu.type, true, false, false, false, true, data.params; x0=x0)\n",
    "\n",
    "    # batch the data (time, targets), train only on model output index 6, plot batch elements\n",
    "    batch = batchDataSolution(neuralFMU,                            # our NeuralFMU model\n",
    "                              t -> FMIZoo.getStateVector(data, t),  # a function returning a start state for a given time point `t`, to determine start states for batch elements\n",
    "                              train_t,                              # data time points\n",
    "                              train_data;                           # data cumulative consumption \n",
    "                              batchDuration=BATCHDUR,               # duration of one batch element\n",
    "                              indicesModel=6:6,                     # model indices to train on (6 equals the state `cumulative consumption`)\n",
    "                              plot=false,                           # don't show intermediate plots (try this outside of Jupyter)\n",
    "                              parameters=data.params,               # use the parameters (map file paths) from *FMIZoo.jl*\n",
    "                              showProgress=showProgress)            # show or don't show progess bar, as specified at the very beginning\n",
    "\n",
    "    # limit the maximum number of solver steps to 1000 * BATCHDUR (longer batch elements get more steps)\n",
    "    # this allows the NeuralFMU to do 10x more steps (average) than the original FMU, but more should not be tolerated (to stiff system)\n",
    "    solverKwargsTrain = Dict{Symbol, Any}(:maxiters => round(Int, 1000*BATCHDUR)) \n",
    "    \n",
    "    # a smaller dispatch for our custom loss function, only taking the solution object\n",
    "    lossFct = (solution::FMU2Solution) -> _lossFct(solution, data, LOSS, LASTWEIGHT)\n",
    "\n",
    "    # selecting a scheduler for training\n",
    "    scheduler = nothing\n",
    "    if SCHEDULER == :Random\n",
    "        # a scheduler that picks a random batch element\n",
    "        scheduler = RandomScheduler(neuralFMU, batch; applyStep=1, plotStep=0)\n",
    "    elseif SCHEDULER == :Sequential\n",
    "        # a scheduler that picks one batch element after another (in chronological order)\n",
    "        scheduler = SequentialScheduler(neuralFMU, batch; applyStep=1, plotStep=0)\n",
    "    elseif SCHEDULER == :LossAccumulation\n",
    "        # a scheduler that picks the element with largest accumulated loss:\n",
    "        # - after every training step, the accumulated loss for every batch element is increased by the current loss value \n",
    "        # - when picking a batch element, the accumulated loss is reset to zero\n",
    "        # - this promotes selecting elements with larger losses more often, but also prevents starving of elements with small losses\n",
    "        scheduler = LossAccumulationScheduler(neuralFMU, batch, lossFct; applyStep=1, plotStep=0, updateStep=1)\n",
    "    else \n",
    "        @error \"Unknown SCHEDULER: ´$(SCHEDULER)´.\"\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    # loss for training, do a simulation run on a batch element taken from the scheduler\n",
    "    loss = p -> FMIFlux.Losses.loss(neuralFMU,                          # the NeuralFMU to simulate\n",
    "                                    batch;                              # the batch to take an element from\n",
    "                                    p=p,                                # the NeuralFMU training parameters (given as input)\n",
    "                                    parameters=data.params,             # the FMU paraemters\n",
    "                                    lossFct=lossFct,                    # our custom loss function\n",
    "                                    batchIndex=scheduler.elementIndex,  # the index of the batch element to take, determined by the choosen scheduler\n",
    "                                    logLoss=true,                       # log losses after every evaluation\n",
    "                                    showProgress=showProgress,          # sho progress bar (or don't)\n",
    "                                    solverKwargsTrain...)               # the solver kwargs defined above\n",
    "\n",
    "    # gather the parameters from the NeuralFMU\n",
    "    params = FMIFlux.params(neuralFMU)\n",
    "\n",
    "    # initialize the scheduler \n",
    "    initialize!(scheduler; parameters=data.params, p=params[1], showProgress=showProgress)\n",
    "    \n",
    "    # initialize Adam optimizer with our hyperparameters\n",
    "    optim = Adam(ETA, (BETA1, BETA2))\n",
    "   \n",
    "    # the actual training\n",
    "    FMIFlux.train!(loss,                            # the loss function for training\n",
    "                   params,                          # the parameters to train\n",
    "                   Iterators.repeated((), steps),   # an iterator repeating `steps` times\n",
    "                   optim;                           # the optimizer to train\n",
    "                   gradient=:ForwardDiff,           # we use *ForwardDiff.jl* for gradient determination\n",
    "                   chunk_size=32,                   # the ForwardDiff chunk size is 32, so 32 parameter sensitivites are determined at a time\n",
    "                   cb=() -> update!(scheduler),     # update the scheduler after every step \n",
    "                   proceed_on_assert=true)          # proceed, even if assertions are thrown, with the next step\n",
    "    \n",
    "    # switch back to the default execution configuration, allocate a new FMU instance for every run, see:\n",
    "    # https://thummeto.github.io/FMI.jl/dev/features/#Execution-Configuration\n",
    "    fmu.executionConfig = FMI.FMIImport.FMU2_EXECUTION_CONFIGURATION_NO_RESET\n",
    "    FMIFlux.finishSolveFMU(neuralFMU.fmu, c, false, true)\n",
    "\n",
    "    # save our result parameters\n",
    "    fmiSaveParameters(neuralFMU, joinpath(@__DIR__, \"params\", \"$(HPRUN)\", \"$(ind).jld2\"))\n",
    "    \n",
    "    # simulate the NeuralFMU on a validation trajectory\n",
    "    resultNFMU = neuralFMU(x0, (data_validation.consumption_t[1], data_validation.consumption_t[end]); parameters=data_validation.params, showProgress=showProgress, maxiters=1e7, saveat=data_validation.consumption_t)\n",
    "\n",
    "    # determine loss on validation data (if the simulation was successfull)\n",
    "    validation_loss = nothing \n",
    "    if resultNFMU.success\n",
    "        validation_loss = _lossFct(resultNFMU, data_validation, :MSE)\n",
    "    end        \n",
    "\n",
    "    # unload FMU\n",
    "    fmiUnload(fmu)\n",
    "\n",
    "    # return the loss (or `nothing` if no loss can be determined)\n",
    "    return validation_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the train function is working for a given set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the train function is working for a set of given hyperparameters\n",
    "#     ([  ETA, BETA1,  BETA2, BATCHDUR, LASTWEIGHT,         SCHEDULER, LOSS], RESSOURCE, INDEX)\n",
    "train!([0.001,   0.9, 0.9999,     10.0,        0.7, :LossAccumulation, :MSE],      10.0,     1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results Discussion\n",
    "After hyperparameter optimization, results can be loaded (one is already preparred if you skipped the optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our FMU (we take one from the FMIZoo.jl, exported with Dymola 2022x)\n",
    "fmu = fmiLoad(\"VLDM\", \"Dymola\", \"2020x\"; type=:ME)\n",
    "\n",
    "# build NeuralFMU\n",
    "neuralFMU = build_NFMU(fmu)\n",
    "\n",
    "# load parameters from hyperparameter optimization\n",
    "fmiLoadParameters(neuralFMU, joinpath(@__DIR__, \"juliacon_2023.jld2\"))\n",
    "\n",
    "# simulate and plot the NeuralFMU\n",
    "resultNFMU = neuralFMU(x0, (tStart, tStop); parameters=data.params, showProgress=showProgress, saveat=tSave) \n",
    "resultFMU = fmiSimulate(fmu, (tStart, tStop); parameters=data.params, showProgress=showProgress, saveat=tSave) \n",
    "\n",
    "# plot the NeuralFMU, original FMU and data (cumulative consumption)\n",
    "fig = plot(resultNFMU; stateIndices=6:6, stateEvents=false, timeEvents=false, label=\"NeuralFMU\", ylabel=\"cumulative consumption [m/s]\")\n",
    "plot!(fig, resultFMU; stateIndices=6:6, values=false, stateEvents=false, timeEvents=false, label=\"FMU\")\n",
    "plot!(fig, data.cumconsumption_t, data.cumconsumption_val, label=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a ready-to-use function that calculates different errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCumulativeConsumption(resultNFMU, resultFMU, data; filename=joinpath(@__DIR__, \"comparision_train_100.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the deviation is small, let's check the last 10% of WLTC focussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCumulativeConsumption(resultNFMU, resultFMU, data; range=(0.9, 1.0), filename=joinpath(@__DIR__, \"comparision_train_10.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we should check the results on validation data: The full WLTC cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start and stop for the validation cycle (full WLTC)\n",
    "tStart_validation = data_validation.cumconsumption_t[1]\n",
    "tStop_validation = data_validation.cumconsumption_t[end]\n",
    "tSave_validation = data_validation.cumconsumption_t\n",
    "\n",
    "# simulate the NeuralFMU on validation data\n",
    "resultNFMU = neuralFMU(x0, (tStart_validation, tStop_validation); parameters=data_validation.params, showProgress=showProgress, saveat=tSave_validation) \n",
    "resultFMU = fmiSimulate(fmu, (tStart_validation, tStop_validation); parameters=data_validation.params, showProgress=showProgress, saveat=tSave_validation) \n",
    "\n",
    "plotCumulativeConsumption(resultNFMU, resultFMU, data_validation; filename=joinpath(@__DIR__, \"comparision_validation_100.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the last 10% ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCumulativeConsumption(resultNFMU, resultFMU, data_validation; range=(0.9, 1.0), filename=joinpath(@__DIR__, \"comparision_validation_10.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we finished, let's finally unload the FMU and invalidate the NeuralFMU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload FMU / invalidate NeuralFMU\n",
    "fmiUnload(fmu)\n",
    "neuralFMU = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Organize as module\n",
    "If you want, you can place all code inside of a module named `NODE_Training`, this simplifies hyper parameter optimization (if you want to do one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hyper parameter optimization, place the code in a `module`\n",
    "# just uncomment the following line (and the one at the very beginning, too!) \n",
    "#end # NODE_Training "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
